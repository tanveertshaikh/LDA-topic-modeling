{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling using Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Vox News corpus is a collection of all Vox articles published before March 21, 2017. Vox Media released this dataset as part of the KDD 2017 Workshop on Data Science + Journalism. Their goal for publishing this dataset was to enable data science researchers to apply various techniques on a news dataset.\n",
    "\n",
    "<b>The dataset consists of 22,994 news articles with their titles, author names, categories, published dates, updated on dates, links to the articles and their short descriptions (8 columns). While visualizing the dataset, I noticed that all the articles are clustered by 185 distinct categories. Out of those articles, 7145 articles were tageed by the category \"The Latest\". It cannot be a coincidence that such a large number of articles would be tagged by a generic category. Hence, I decided to address this problem by unsupervised learning because the categories of articles cannot be predicted beforehand neither the articles can be tagged by their categories in the training dataset.</b>\n",
    "\n",
    "<b>We get a crude idea of the article by just skimming through the category of the article. Hence, topic modeling is useful for categorizing or ranking articles which are remaining to be read by an individual. Moreover, clustering of articles based on topics also enable them to be organized by groups of similar topics inside a database. This simplifies the collective analysis of such Big Data especially in the field of News and Journalism where an enormous amount of data is archived and retrieved only when needed. Categorical clustering will also make information retrieval quicker and more efficient.</b>\n",
    "\n",
    "We can analyze the title, short description and the body of these 7145 articles and predict their categories by using Topic Modeling.\n",
    "\n",
    "In reality, analyzing the body would drastically improve the topic model. However, due to time constraints and proclivity towards minimalism, I have decided to drop the body column entirely. Also, parsing html tags in the body of articles would be a time-consuming task in itself.\n",
    "\n",
    "In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: \"dog\" and \"bone\" will appear more often in documents about dogs, \"cat\" and \"meow\" will appear in documents about cats, and \"the\" and \"is\" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10% about cats and 90% about dogs, there would probably be about 9 times more dog words than cat words. The \"topics\" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.\n",
    "\n",
    "Topic models are also referred to as probabilistic topic models as they are based on probabilistic graphical modeling, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics. [Source: https://en.wikipedia.org/wiki/Topic_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The necessary python libraries and packages like numpy, pandas, matplotlib and scikit-learn have been imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# numpy, pandas, matplotlib and regular expressions (data science essentials)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# spacy\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_sm\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import words\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# styling\n",
    "pd.set_option('display.max_columns',150)\n",
    "plt.style.use('bmh')\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)  Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bubble chart quantifies the number of articles written by different authors.  [Source: https://data.world/elenadata/vox-articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 1](../bin/resources/articles-per-author.png \"Figure 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph signifies the gradual increase in the number of articles being published during each month. However, the average articles published in the months of 2017 and 2016 seems to be the similar.  [Source: https://data.world/elenadata/vox-articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 2](../bin/resources/articles-by-month.png \"Figure 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire dataset consists of a total of 185 distinct topics. This bubble plot shows records grouped by category. We can observe that the category \"The Latest\" has the maximum number of records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 3](../bin/resources/records-by-category.png \"Figure 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This bar graph tells us the distribution of records around topics and also around different authors who have written about the same topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 4](../bin/resources/records-by-category-&-author.png \"Figure 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of algorithms developed for topic modeling which use singular value decomposition (SVD) and the method of moments. These algorithms are listed below:\n",
    "<ul>Explicit semantic analysis</ul>\n",
    "<ul>Latent semantic analysis</ul>\n",
    "<ul>Latent Dirichlet Allocation (LDA)</ul>\n",
    "<ul>Hierarchical Dirichlet process</ul>\n",
    "<ul>Non-Negative Matrix Factorization (NMF)</ul>\n",
    "\n",
    "I decided to use LDA as it is widely praised topic modeling technique by researchers and data scientists. Owing to my Data Mining project, I also had prior experience on working with Gensim library in Python which has a robust LDA model. LDA is a kind of probabilistic model that exploits similarity between data and extracts inference from the resulting analysis.\n",
    "\n",
    "In natural language processing, Latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. For example, if observations are words collected into documents, it posits that each document is a mixture of a small number of topics and that each word's creation is attributable to one of the document's topics. LDA is an example of a topic model and was first presented as a graphical model for topic discovery by David Blei, Andrew Ng, and Michael I. Jordan in 2003. Essentially the same model was also proposed independently by J. K. Pritchard, M. Stephens, and P. Donnelly in the study of population genetics in 2000. Both papers have been highly influential, with 19858 and 20416 citations respectively by August 2017.  [Source: https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation]\n",
    "\n",
    "In simple words, one can say that LDA converts a set of documents into a set of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell will auto-download the required NLTK modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tanveershaikh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tanveershaikh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "warnings.simplefilter('ignore')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "nlp = spacy.load('en')\n",
    "# nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NLTK, I am creating a corpus of English words and also an object of the lemmatizer is being created using WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization and Global Variables\n",
    "dictionary = dict.fromkeys(words.words(), None)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "\n",
    "# List for filtering out stop words\n",
    "en_stop = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Reading the dataset (dsjVoxArticles.tsv) - Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "News Article topic modeling is an unsupervised machine learning method that helps us discover hidden semantic structures in an article, that allows us to learn topic representations of articles in a corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is being fetched from the data.world URL and converted into a Pandas DataFrame in the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = \"https://query.data.world/s/ee6arp6cngynnoj4hvyuhckn3tb4hj\"\n",
    "df = pd.read_csv(url, delimiter = '\\t', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting only the articles having category as 'The Latest' and dropping all other articles which have their correct categories would be unfair because we are dropping our training data. Hence, I am keeping all 22,994 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Exploratory Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section deals with exploring and analyzing the dataset. It will give us a deeper understanding of the dataset by making us familiar with all the rows and columns of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_on</th>\n",
       "      <th>slug</th>\n",
       "      <th>blurb</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bitcoin is down 60 percent this year. Here's w...</td>\n",
       "      <td>Timothy B. Lee</td>\n",
       "      <td>Business &amp; Finance</td>\n",
       "      <td>2014-03-31 14:01:30</td>\n",
       "      <td>2014-12-16 16:37:36</td>\n",
       "      <td>http://www.vox.com/2014/3/31/5557170/bitcoin-b...</td>\n",
       "      <td>Bitcoins have lost more than 60 percent of the...</td>\n",
       "      <td>&lt;p&gt;The markets haven't been kind to&lt;span&gt; &lt;/sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6 health problems marijuana could treat better...</td>\n",
       "      <td>German Lopez</td>\n",
       "      <td>War on Drugs</td>\n",
       "      <td>2014-03-31 15:44:21</td>\n",
       "      <td>2014-11-17 00:20:33</td>\n",
       "      <td>http://www.vox.com/2014/3/31/5557700/six-probl...</td>\n",
       "      <td>Medical marijuana could fill gaps that current...</td>\n",
       "      <td>&lt;p&gt;Twenty states have so far legalized the med...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9 charts that explain the history of global we...</td>\n",
       "      <td>Matthew Yglesias</td>\n",
       "      <td>Business &amp; Finance</td>\n",
       "      <td>2014-04-10 13:30:01</td>\n",
       "      <td>2014-12-16 15:47:02</td>\n",
       "      <td>http://www.vox.com/2014/4/10/5561608/9-charts-...</td>\n",
       "      <td>These nine charts from Thomas Piketty's new bo...</td>\n",
       "      <td>&lt;p&gt;Thomas Piketty's book &lt;i&gt;Capital in the 21s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Remember when legal marijuana was going to sen...</td>\n",
       "      <td>German Lopez</td>\n",
       "      <td>Criminal Justice</td>\n",
       "      <td>2014-04-03 23:25:55</td>\n",
       "      <td>2014-05-06 21:58:42</td>\n",
       "      <td>http://www.vox.com/2014/4/3/5563134/marijuana-...</td>\n",
       "      <td>Three months after legalizing marijuana, Denve...</td>\n",
       "      <td>&lt;p&gt;&lt;span&gt;When Colorado legalized recreational ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Obamacare succeeded for one simple reason: it'...</td>\n",
       "      <td>Sarah Kliff</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>2014-04-01 20:26:14</td>\n",
       "      <td>2014-11-18 15:09:14</td>\n",
       "      <td>http://www.vox.com/2014/4/1/5570780/the-two-re...</td>\n",
       "      <td>After a catastrophic launch, Obamacare still s...</td>\n",
       "      <td>&lt;p&gt;There's a very simple reason that Obamacare...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title            author  \\\n",
       "0  Bitcoin is down 60 percent this year. Here's w...    Timothy B. Lee   \n",
       "1  6 health problems marijuana could treat better...      German Lopez   \n",
       "2  9 charts that explain the history of global we...  Matthew Yglesias   \n",
       "3  Remember when legal marijuana was going to sen...      German Lopez   \n",
       "4  Obamacare succeeded for one simple reason: it'...       Sarah Kliff   \n",
       "\n",
       "             category       published_date           updated_on  \\\n",
       "0  Business & Finance  2014-03-31 14:01:30  2014-12-16 16:37:36   \n",
       "1        War on Drugs  2014-03-31 15:44:21  2014-11-17 00:20:33   \n",
       "2  Business & Finance  2014-04-10 13:30:01  2014-12-16 15:47:02   \n",
       "3    Criminal Justice  2014-04-03 23:25:55  2014-05-06 21:58:42   \n",
       "4         Health Care  2014-04-01 20:26:14  2014-11-18 15:09:14   \n",
       "\n",
       "                                                slug  \\\n",
       "0  http://www.vox.com/2014/3/31/5557170/bitcoin-b...   \n",
       "1  http://www.vox.com/2014/3/31/5557700/six-probl...   \n",
       "2  http://www.vox.com/2014/4/10/5561608/9-charts-...   \n",
       "3  http://www.vox.com/2014/4/3/5563134/marijuana-...   \n",
       "4  http://www.vox.com/2014/4/1/5570780/the-two-re...   \n",
       "\n",
       "                                               blurb  \\\n",
       "0  Bitcoins have lost more than 60 percent of the...   \n",
       "1  Medical marijuana could fill gaps that current...   \n",
       "2  These nine charts from Thomas Piketty's new bo...   \n",
       "3  Three months after legalizing marijuana, Denve...   \n",
       "4  After a catastrophic launch, Obamacare still s...   \n",
       "\n",
       "                                                body  \n",
       "0  <p>The markets haven't been kind to<span> </sp...  \n",
       "1  <p>Twenty states have so far legalized the med...  \n",
       "2  <p>Thomas Piketty's book <i>Capital in the 21s...  \n",
       "3  <p><span>When Colorado legalized recreational ...  \n",
       "4  <p>There's a very simple reason that Obamacare...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints out the first 5 rows of data in the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_on</th>\n",
       "      <th>slug</th>\n",
       "      <th>blurb</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23019</th>\n",
       "      <td>Marijuana legalization opponents warned teen p...</td>\n",
       "      <td>German Lopez</td>\n",
       "      <td>The Latest</td>\n",
       "      <td>2017-03-21 19:30:01</td>\n",
       "      <td>2017-03-21 19:51:00</td>\n",
       "      <td>http://www.vox.com/policy-and-politics/2017/3/...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;p id=\"6OljE3\"&gt;So far, &lt;a href=\"http://www.vox...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23020</th>\n",
       "      <td>4 ways the House health care vote could go dow...</td>\n",
       "      <td>Andrew Prokop</td>\n",
       "      <td>The Latest</td>\n",
       "      <td>2017-03-21 21:41:12</td>\n",
       "      <td>2017-03-21 23:46:25</td>\n",
       "      <td>http://www.vox.com/policy-and-politics/2017/3/...</td>\n",
       "      <td>This Thursday should be an eventful day.</td>\n",
       "      <td>&lt;p id=\"5WuiOu\"&gt;House Speaker Paul Ryan still a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23021</th>\n",
       "      <td>In search of Forrest Fenn's treasure</td>\n",
       "      <td>Zachary Crockett</td>\n",
       "      <td>First Person</td>\n",
       "      <td>2017-02-28 12:33:27</td>\n",
       "      <td>2017-02-28 14:12:31</td>\n",
       "      <td>http://www.vox.com/a/fenn-treasure-hunt-map</td>\n",
       "      <td></td>\n",
       "      <td>&lt;div class=\"restricted\"&gt; \\n \\n      &lt;p class=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23022</th>\n",
       "      <td>Oscars 2017: every movie nominated for an Acad...</td>\n",
       "      <td>Sarah Frostenson</td>\n",
       "      <td>The Latest</td>\n",
       "      <td>2017-02-23 20:20:01</td>\n",
       "      <td>2017-02-26 14:10:56</td>\n",
       "      <td>http://www.vox.com/a/oscars-2017-movies-nominees</td>\n",
       "      <td>Yes, even 13 Hours: The Secret Soldiers of Ben...</td>\n",
       "      <td>&lt;h2 id=\"RCAyzl\"&gt;&lt;a href=\"http://www.imdb.com/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23023</th>\n",
       "      <td>Transcript: President Trumpâ€™s speech to Cong...</td>\n",
       "      <td>Vox Staff</td>\n",
       "      <td>Congress</td>\n",
       "      <td>2017-03-01 01:06:07</td>\n",
       "      <td>2017-03-01 04:06:34</td>\n",
       "      <td>http://www.vox.com/a/trump-speech-transcript-j...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt;President Trump took a trip up Pennsylvani...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title            author  \\\n",
       "23019  Marijuana legalization opponents warned teen p...      German Lopez   \n",
       "23020  4 ways the House health care vote could go dow...     Andrew Prokop   \n",
       "23021               In search of Forrest Fenn's treasure  Zachary Crockett   \n",
       "23022  Oscars 2017: every movie nominated for an Acad...  Sarah Frostenson   \n",
       "23023  Transcript: President Trumpâ€™s speech to Cong...         Vox Staff   \n",
       "\n",
       "           category       published_date           updated_on  \\\n",
       "23019    The Latest  2017-03-21 19:30:01  2017-03-21 19:51:00   \n",
       "23020    The Latest  2017-03-21 21:41:12  2017-03-21 23:46:25   \n",
       "23021  First Person  2017-02-28 12:33:27  2017-02-28 14:12:31   \n",
       "23022    The Latest  2017-02-23 20:20:01  2017-02-26 14:10:56   \n",
       "23023      Congress  2017-03-01 01:06:07  2017-03-01 04:06:34   \n",
       "\n",
       "                                                    slug  \\\n",
       "23019  http://www.vox.com/policy-and-politics/2017/3/...   \n",
       "23020  http://www.vox.com/policy-and-politics/2017/3/...   \n",
       "23021        http://www.vox.com/a/fenn-treasure-hunt-map   \n",
       "23022   http://www.vox.com/a/oscars-2017-movies-nominees   \n",
       "23023  http://www.vox.com/a/trump-speech-transcript-j...   \n",
       "\n",
       "                                                   blurb  \\\n",
       "23019                                                      \n",
       "23020           This Thursday should be an eventful day.   \n",
       "23021                                                      \n",
       "23022  Yes, even 13 Hours: The Secret Soldiers of Ben...   \n",
       "23023                                                      \n",
       "\n",
       "                                                    body  \n",
       "23019  <p id=\"6OljE3\">So far, <a href=\"http://www.vox...  \n",
       "23020  <p id=\"5WuiOu\">House Speaker Paul Ryan still a...  \n",
       "23021   <div class=\"restricted\"> \\n \\n      <p class=...  \n",
       "23022   <h2 id=\"RCAyzl\"><a href=\"http://www.imdb.com/...  \n",
       "23023   <p>President Trump took a trip up Pennsylvani...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prints out the last 5 rows of data in the dataset\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>published_date</th>\n",
       "      <th>updated_on</th>\n",
       "      <th>slug</th>\n",
       "      <th>blurb</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>23024</td>\n",
       "      <td>23023</td>\n",
       "      <td>23013</td>\n",
       "      <td>23013</td>\n",
       "      <td>23013</td>\n",
       "      <td>23013</td>\n",
       "      <td>23013</td>\n",
       "      <td>23013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>22977</td>\n",
       "      <td>825</td>\n",
       "      <td>185</td>\n",
       "      <td>22825</td>\n",
       "      <td>22474</td>\n",
       "      <td>23013</td>\n",
       "      <td>20043</td>\n",
       "      <td>23013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Republican debate 2016 live stream: time, TV s...</td>\n",
       "      <td>German Lopez</td>\n",
       "      <td>The Latest</td>\n",
       "      <td>2016-07-13 12:00:03</td>\n",
       "      <td>2016-11-16 21:01:35</td>\n",
       "      <td>http://www.vox.com/2015/1/30/7952719/deflatega...</td>\n",
       "      <td></td>\n",
       "      <td>&lt;p&gt;In 1937, shortly after relocating permanent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>7</td>\n",
       "      <td>2243</td>\n",
       "      <td>7152</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2729</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title        author  \\\n",
       "count                                               23024         23023   \n",
       "unique                                              22977           825   \n",
       "top     Republican debate 2016 live stream: time, TV s...  German Lopez   \n",
       "freq                                                    7          2243   \n",
       "\n",
       "          category       published_date           updated_on  \\\n",
       "count        23013                23013                23013   \n",
       "unique         185                22825                22474   \n",
       "top     The Latest  2016-07-13 12:00:03  2016-11-16 21:01:35   \n",
       "freq          7152                    9                   24   \n",
       "\n",
       "                                                     slug  blurb  \\\n",
       "count                                               23013  23013   \n",
       "unique                                              23013  20043   \n",
       "top     http://www.vox.com/2015/1/30/7952719/deflatega...          \n",
       "freq                                                    1   2729   \n",
       "\n",
       "                                                     body  \n",
       "count                                               23013  \n",
       "unique                                              23013  \n",
       "top     <p>In 1937, shortly after relocating permanent...  \n",
       "freq                                                    1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summary statistics about the data column-wise\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Training and Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set with all 20003 rows\n",
    "df_train = df.copy()\n",
    "\n",
    "# Duplicate the original dataset\n",
    "df_test = df.copy()\n",
    "\n",
    "# Drop rows with category \"The Latest\" from our training dataset\n",
    "df_train = df_train.loc[df_train['category'] != 'The Latest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, the data pre-processing steps include dropping the irrelevant columns from the dataframe and then dropping the rows having any of the values as NaN.\n",
    "\n",
    "But before doing that step, empty cell locations are being checked or the ones having whitespaces. These rows are marked and dropped entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning the dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Cleaning the dataset...\")\n",
    "columns_train = ['author','category','published_date','updated_on','slug','body']\n",
    "df_train.drop(columns_train, axis = 1, inplace = True)\n",
    "\n",
    "columns_test = ['author','published_date','updated_on','slug','body']\n",
    "df_test.drop(columns_test, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23024, 8)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(df.shape)\n",
    "print(df.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to drop missing values as we have a large number of records to train on and the number of records having at least 1 value missing is negligible. Hence, it will only have minuscule effect on our model’s performance which can be neglected. <br>\n",
    "\n",
    "<br>I am deleting the author, published_date and updated_on columns as they are irrelevant to my end goal, which is, topic modeling using the title and blurb (short description).<br>\n",
    "\n",
    "<br>I have also decided to delete the slug and body columns as this is just a naive implementation of topic modeling. I will have to consider those two columns after completing this project to make my topic modeling more coherent.\n",
    "Also, I am dropping the category column as I am trying to determine that attribute itself and unsupervised learning does not require the training labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing missing values...\n"
     ]
    }
   ],
   "source": [
    "print(\"Removing missing values...\")\n",
    "df_train['blurb'].replace(' ', np.nan, inplace = True)\n",
    "df_train.dropna(axis = 0, how = 'any', inplace = True)\n",
    "\n",
    "df_test['blurb'].replace(' ', np.nan, inplace = True)\n",
    "df_test.dropna(axis = 0, how = 'any', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am performing the operation of cleaning up weird characters from the dataframe. These characters exist because the string data was decoded in another format and is now being encoded in UTF-8 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking values...\n"
     ]
    }
   ],
   "source": [
    "df_train.apply(lambda x: x.apply(lambda y: y.strip() if type(y) == type('') else y), axis=0)\n",
    "\n",
    "df_train['blurb'] = df_train['blurb'].str.replace('â€™',\"'\").str.replace('â€”',\"-\").str.replace('â€œ','\"').str.replace('â€','\"')\n",
    "df_train['blurb'] = df_train['blurb'].str.strip()\n",
    "df_train['blurb'] = df_train['blurb'].apply(lambda x: x.strip())\n",
    "\n",
    "df_train['title'] = df_train['title'].str.replace('â€™',\"'\").str.replace('â€”',\"-\").str.replace('â€œ','\"').str.replace('â€','\"')\n",
    "df_train['title'] = df_train['title'].str.strip()\n",
    "df_train['title'] = df_train['title'].apply(lambda x: x.strip())\n",
    "\n",
    "# Same operation on test dataset\n",
    "df_test.apply(lambda x: x.apply(lambda y: y.strip() if type(y) == type('') else y), axis=0)\n",
    "\n",
    "df_test['blurb'] = df_test['blurb'].str.replace('â€™',\"'\").str.replace('â€”',\"-\").str.replace('â€œ','\"').str.replace('â€','\"')\n",
    "df_test['blurb'] = df_test['blurb'].str.strip()\n",
    "df_test['blurb'] = df_test['blurb'].apply(lambda x: x.strip())\n",
    "\n",
    "df_test['title'] = df_test['title'].str.replace('â€™',\"'\").str.replace('â€”',\"-\").str.replace('â€œ','\"').str.replace('â€','\"')\n",
    "df_test['title'] = df_test['title'].str.strip()\n",
    "df_test['title'] = df_test['title'].apply(lambda x: x.strip())\n",
    "\n",
    "# Checking Values\n",
    "print(\"Checking values...\")\n",
    "# print(df_train.at[23003, 'blurb'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I am keeping only the distinct (unique) values of titles as well as of the blurb and dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train = df_train.drop_duplicates('blurb')\n",
    "df_train = df_train.drop_duplicates('title')\n",
    "\n",
    "df_test = df_test.drop_duplicates('blurb')\n",
    "df_test = df_test.drop_duplicates('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting our dataset into a collection of 5495 documents with just 1 column consisting of title concatenated with blurb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['documents'] = df_train['title'].map(str) + '. ' + df_train['blurb'].map(str)\n",
    "\n",
    "df_test['documents'] = df_test['title'].map(str) + '. ' + df_test['blurb'].map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['title','blurb']\n",
    "df_train.drop(columns, axis = 1, inplace = True)\n",
    "\n",
    "df_test.drop(columns, axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting only the articles having category as 'The Latest' and dropping all other articles which have their correct categories to build our test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14554, 1)\n",
      "2\n",
      "(5455, 1)\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Test set with only approximately 5554 articles which have category as \"The Latest\"\n",
    "df_test = df_test.loc[df_test['category'] == 'The Latest']\n",
    "df_test.drop('category', axis = 1, inplace = True)\n",
    "\n",
    "print(df_train.shape)\n",
    "print(df_train.ndim)\n",
    "\n",
    "print(df_test.shape)\n",
    "print(df_test.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA Implementation Process:<br>\n",
    "<br>The number of topics should have been already decided even if we're not sure what the topics are.\n",
    "<br>Each document is represented as a distribution over topics.\n",
    "<br>Each topic is represented as a distribution over words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Text Cleaning and Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function cleans our text and returns a list of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = English()\n",
    "def tokenize(text):\n",
    "    lda_tokens = []\n",
    "    tokens = parser(text)\n",
    "    for token in tokens:\n",
    "        if token.orth_.isspace():\n",
    "            continue\n",
    "        elif token.like_url:\n",
    "            lda_tokens.append('URL')\n",
    "        elif token.orth_.startswith('@'):\n",
    "            lda_tokens.append('SCREEN_NAME')\n",
    "        else:\n",
    "            lda_tokens.append(token.lower_)\n",
    "    return lda_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will use NLTK's Wordnet to find the synonyms, antonyms and the meanings of words. In addition, WordNetLemmatizer() will give us the root word of a token. (similar to stemming but better)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lemma(word):\n",
    "    lemma = wn.morphy(word)\n",
    "    if lemma is None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am trying to make my code modularized by defining functions for specific tasks. \n",
    "\n",
    "Like, I defined this function to prepare the text for topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_lda(text):\n",
    "    tokens = tokenize(text)\n",
    "    tokens = [token for token in tokens if len(token) > 4]\n",
    "    tokens = [token for token in tokens if token not in en_stop]\n",
    "    tokens = [get_lemma(token) for token in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function opens our data frame, reads each row sequentially; and for each row, prepares text for LDA, and then adds it to a list. This function calls the helper function prepare_text_for_lda(text) which in-turn calls all the other functions I defined above\n",
    "\n",
    "The following cell shows how each of our document is being tokenized. Each and every document is getting converted into a list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14554/14554 [00:16<00:00, 878.86it/s]\n"
     ]
    }
   ],
   "source": [
    "text_data = []\n",
    "for row in tqdm(df_train['documents']):\n",
    "    tokens = prepare_text_for_lda(row)\n",
    "    #if random.random() > .99:\n",
    "        #print(tokens)\n",
    "    text_data.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word dictionary and corpus needed for topic modeling are created and saved on disk for further usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, I am creating a dictionary from our data, then I am converting it into a bag-of-words corpus and then saving the dictionary and corpus for future use using pickle library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "pickle.dump(corpus, open('../bin/resources/corpus.pkl', 'wb'))\n",
    "dictionary.save('../bin/resources/dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10) Building the Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the number of topics in the articles will be proportional to the number of articles in the dataset. For our original dataset consisting of 22,994 articles, we had around 185 distinct topics. Proportionally, for our subset of articles; it can be inferred that we will have around 227 unique topics. So , the total topics to be trained on becomes 227\n",
    "\n",
    "LDA is finding 227 topics from our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TOPICS = 100\n",
    "start_time = time.time()\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, \n",
    "                                           num_topics=NUM_TOPICS, \n",
    "                                           random_state=89, \n",
    "                                           update_every=1,  \n",
    "                                           id2word=dictionary, \n",
    "                                           passes=42, \n",
    "                                           alpha='auto', \n",
    "                                           per_word_topics=True)\n",
    "\n",
    "ldamodel.save('../bin/resources/model100.gensim')\n",
    "train_time = time.time() - start_time\n",
    "print(\"Training Time --- %s seconds \" % (round(train_time, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above LDA model is built with 227 different topics where each topic is a combination of keywords and each keyword contributes a certain weightage to the topic. You can see the keywords for each topic and the weightage(importance) of each keyword using lda_model.print_topics() as shown next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ldamodel.print_topics()\n",
    "for topic in topics:\n",
    "    pp.pprint(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic 5 includes words like “history”, \"netflix\", \"play\" and \"comedy\" which sounds like a topic related to movies/entertainment.\n",
    "\n",
    "Topic 96 includes words like “election”, “hillary”, “clinton”, \"campaign\" and “republican”; it is definitely a politics related topic.\n",
    "\n",
    "Topic 40 includes words like “obamacare”, “health”, “insurance”, \"liberal\" and “coverage”, sounds like a topic related to healthcare and health insurance. and so on.\n",
    "\n",
    "With LDA, we can see that different documents have different topics, and the discriminations are obvious."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', ldamodel.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=ldamodel, texts=text_data, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df_test['documents']:\n",
    "    test_doc = prepare_text_for_lda(test_doc)\n",
    "    test_doc_bow = dictionary.doc2bow(test_doc)\n",
    "    print(test_doc_bow)\n",
    "    print(ldamodel.get_document_topics(test_doc_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
